# From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception

[cite_start]This repository contains the official code and data for **CogIP-Bench** (Cognition Image Property Benchmark) and the associated alignment methods described in the paper *"From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images"*[cite: 1, 6].

[cite_start]While Multimodal Large Language Models (MLLMs) excel at identifying "what" is in an image, they often struggle to understand "how" an image feels to a human observer[cite: 4]. This project addresses that gap by evaluating and aligning models on subjective cognitive properties.

## ðŸ§  Project Overview

[cite_start]This framework focuses on four key dimensions of visual cognition[cite: 17, 145]:

1.  [cite_start]**Aesthetics:** Visual appeal, harmony, and artistic value[cite: 146].
2.  [cite_start]**Funniness:** Unexpected visual incongruity and humor[cite: 150].
3.  [cite_start]**Emotional Valence:** The positive or negative emotional tone evoked by the image[cite: 154].
4.  [cite_start]**Memorability:** How likely an image is to be remembered[cite: 161].

We provide tools for:
* [cite_start]**Benchmarking:** Evaluating MLLMs (Qwen, Llama, Gemma) against human judgment[cite: 177].
* [cite_start]**Alignment (SFT):** A training pipeline using **Soft-Label Loss** and a **"Describe-then-Predict"** strategy to teach models subjective cognition[cite: 197, 202].
* [cite_start]**Generation:** Leveraging the aligned backbone to guide image generation (via Qwen-Image) toward specific cognitive traits[cite: 30].

---

## ðŸ“‚ Directory Structure

The repository is organized into four main modules matching the pipeline described in the paper.

```text
MLLM_Cognition_Alignment
â”œâ”€â”€ data/                               # Dataset and Ground Truths
â”‚   â””â”€â”€ cognition/
â”‚       â”œâ”€â”€ cognition_images/           # Raw image files
â”‚       â”œâ”€â”€ cognition_scores/           # Ground truth scores (Aesthetics, Funniness, etc.)
â”‚       â”œâ”€â”€ test_msg_file/              # Formatted evaluation files for different models
â”‚       â”‚   â”œâ”€â”€ Aesthetics/
â”‚       â”‚   â”œâ”€â”€ Emotional_Valence/
â”‚       â”‚   â”œâ”€â”€ Funniness/
â”‚       â”‚   â””â”€â”€ Memorability/
[cite_start]â”‚       â”œâ”€â”€ cognition_training.json     # SFT Dataset with "Describe-then-Predict" prompts [cite: 198]
[cite_start]â”‚       â””â”€â”€ training_grpo.json          # RL dataset for GRPO experiments 
â”‚
[cite_start]â”œâ”€â”€ evaluation/                         # Benchmarking Scripts [cite: 172]
â”‚   â”œâ”€â”€ gemma/                          # Eval scripts for Gemma-3 variants
â”‚   â”œâ”€â”€ llama/                          # Eval scripts for Llama-3.2-Vision
â”‚   â””â”€â”€ qwen/                           # Eval scripts for Qwen2/2.5-VL
â”‚
[cite_start]â”œâ”€â”€ qwen-image/                         # Downstream Application: Image Generation [cite: 232]
â”‚   â”œâ”€â”€ prompts/                        # Prompts for generating cognition-aligned images
â”‚   â”œâ”€â”€ batch_gene_image.py             # Inference script for image generation
â”‚   â””â”€â”€ run_batch.sh                    # Batch execution script
â”‚
[cite_start]â”œâ”€â”€ sft/                                # Supervised Fine-Tuning Pipeline [cite: 194]
â”‚   â”œâ”€â”€ gemma/                          # Training code for Gemma
â”‚   â”œâ”€â”€ llama/                          # Training code for Llama
â”‚   â””â”€â”€ qwen/                           # Training code for Qwen
â”‚       â”œâ”€â”€ scripts/                    # Launch scripts (Deepspeed/Accelerate)
â”‚       â””â”€â”€ src/                        # Source code for Soft-Label Loss implementation
â”‚
â”œâ”€â”€ environment.yaml                    # Conda environment setup
â””â”€â”€ requirements.txt                    # Python dependencies
```
---

## âš™ï¸ Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/MLLM_Cognition_Alignment.git](https://github.com/your-username/MLLM_Cognition_Alignment.git)
    cd MLLM_Cognition_Alignment
    ```

2.  **Create the environment:**
    ```bash
    conda env create -f environment.yaml
    conda activate cognition_align
    ```

---

## ðŸš€ Usage Guide

### 1. Data Preparation (`data/`)

The `data` folder contains the CogIP-Bench dataset components:

* **`cognition_training.json`**: Contains the training split (**3,200 examples**) formatted with the **"Describe-then-Predict"** prompts.
* **`training_grpo.json`**: Data used for the reinforcement learning (**Group Relative Policy Optimization**) ablation studies.
* **`test_msg_file/`**: Contains `.json` files pre-formatted for inference on the test split (**480 examples**).

### 2. Supervised Fine-Tuning (`sft/`)

We employ a custom SFT pipeline that uses **Soft-Label Loss** to handle the numerical nature of the scores. The code handles the conversion of regression targets into soft probability distributions over token space.

To train a model (e.g., Qwen2.5-VL), navigate to the relevant directory and run the script:

```bash
cd sft/qwen
bash scripts/finetune_lora.sh
```
### 3. Evaluation (`evaluation/`)

To benchmark a model's performance on the 4 cognitive dimensions:

1.  Navigate to the specific model folder (e.g., `evaluation/gemma`).
2.  Run the evaluation script which loads the model and iterates through the `test_msg_file`.

```bash
cd evaluation/gemma
bash cog_test.sh
```
> **Note:** Ensure you configure the path to `cognition_training.json` in the script.

### 4. Image Generation (`qwen-image/`)

This module demonstrates the **transferability of cognitive alignment**. It uses the SFT-aligned MLLM as the backbone for the Qwen-Image pipeline to generate images with specific emotional or aesthetic qualities.

```bash
cd qwen-image
bash run_batch.sh
```

## ðŸ“Š Methodology Highlights

* **Describe-then-Predict:** We force the model to first generate a **descriptive label** (e.g., "very high aesthetic") before predicting the float score. This leverages the LLM's reasoning capabilities.
* **Soft-Label Loss:** Standard Cross-Entropy treats numbers as independent tokens. We implement a **soft-label distribution (triangular function)** to preserve numerical relationships during training, ensuring the model is penalized proportionally to the distance from the ground truth score.